{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name  diractor scriptwriter        actor  \\\n",
      "0           肖申克的救赎  弗兰克·德拉邦特    弗兰克·德拉邦特       蒂姆·罗宾斯    \n",
      "1             霸王别姬       陈凯歌          芦苇          张国荣    \n",
      "2             阿甘正传  罗伯特·泽米吉斯      艾瑞克·罗斯       汤姆·汉克斯    \n",
      "3            泰坦尼克号   詹姆斯·卡梅隆      詹姆斯·卡梅隆  莱昂纳多·迪卡普里奥    \n",
      "4          这个杀手不太冷     吕克·贝松        吕克·贝松        让·雷诺    \n",
      "..             ...       ...          ...          ...   \n",
      "245            我爱你       秋昌旼          姜草          宋在浩    \n",
      "246          完美陌生人   保罗·杰诺维塞     保罗·杰诺维塞       马可·贾利尼    \n",
      "247         花束般的恋爱      土井裕泰         坂元裕二        菅田将晖    \n",
      "248             弱点  约翰·李·汉考克    约翰·李·汉考克      桑德拉·布洛克    \n",
      "249  哈利·波特与死亡圣器(上)     大卫·叶茨    史蒂夫·克洛夫斯    丹尼尔·雷德克里夫    \n",
      "\n",
      "                     country score     vote               type  \n",
      "0                         美国   9.7  2683748            剧情 / 犯罪  \n",
      "1                中国大陆 / 中国香港   9.6  1990775       剧情 / 爱情 / 同性  \n",
      "2                         美国   9.5  2015476            剧情 / 爱情  \n",
      "3      美国 / 墨西哥 / 澳大利亚 / 加拿大   9.4  1974520       剧情 / 爱情 / 灾难  \n",
      "4                    法国 / 美国   9.4  2159294       剧情 / 动作 / 犯罪  \n",
      "..                       ...   ...      ...                ...  \n",
      "245  www.iloveyou2011.co.kr/   9.1   154825            剧情 / 爱情  \n",
      "246                      意大利   8.5   496117            剧情 / 喜剧  \n",
      "247              hana-koi.jp   8.6   484372            剧情 / 爱情  \n",
      "248                       美国   8.7   270782  剧情 / 家庭 / 传记 / 运动  \n",
      "249                  英国 / 美国   8.5   509173            奇幻 / 冒险  \n",
      "\n",
      "[250 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# define a function to get information from website\n",
    "def get_url_info(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'}\n",
    "    try:\n",
    "        url_info = requests.get(url,headers=headers)\n",
    "        url_soup = BeautifulSoup(url_info.text, 'html.parser')\n",
    "        return url_soup\n",
    "    except:\n",
    "        print('sorry')\n",
    "\n",
    "\n",
    "# information list\n",
    "names = [] \n",
    "diractors = []\n",
    "script_writers = []\n",
    "actors = []\n",
    "scores = []\n",
    "types = []\n",
    "countries = []\n",
    "votes = []\n",
    "\n",
    "# get the Douban top 250 films information\n",
    "for i in range(10):\n",
    "    film_page_url = 'https://movie.douban.com/top250?start={}&filter='.format(i*25) # the url of every page\n",
    "    time.sleep(random.randint(1,3)) \n",
    "    page_info = get_url_info(film_page_url)\n",
    "\n",
    "    # get the url of every film specific introduction\n",
    "    \n",
    "    film_selected_info = page_info.find_all(attrs={'class':'hd'})\n",
    "    film_list = [info.a['href'] for info in film_selected_info]\n",
    "\n",
    "    # get the specific information\n",
    "    for film in film_list:\n",
    "        time.sleep(random.randint(1,3))\n",
    "        specific_film_info = get_url_info(film)\n",
    "\n",
    "        names.append(specific_film_info.find(attrs={'property':'v:itemreviewed'}).text.split(' ')[0])\n",
    "        diractors.append(specific_film_info.find(attrs={'id':'info'}).text.split('\\n')[1:11][0].split(': ')[1])\n",
    "\n",
    "        script_writers.append(specific_film_info.find(attrs={'id':'info'}).text.split('\\n')[1:11][1].split(': ')[1].split('/')[0])\n",
    "        actors.append(specific_film_info.find(attrs={'id':'info'}).text.split('\\n')[1:11][2].split(': ')[1].split('/')[0])\n",
    "\n",
    "        types.append(specific_film_info.find(attrs={'id':'info'}).text.split('\\n')[1:11][3].split(': ')[1])\n",
    "        countries.append(specific_film_info.find(attrs={'id':'info'}).text.split('\\n')[1:11][4].split(': ')[1])\n",
    "\n",
    "        scores.append(specific_film_info.find(attrs={'property': 'v:average'}).text)\n",
    "        votes.append(specific_film_info.find(attrs={'property': 'v:votes'}).text)\n",
    "\n",
    "raw_data = {'name':names, 'diractor':diractors, 'scriptwriter':script_writers, 'actor':actors, 'country':countries, 'score':scores, 'vote':votes, 'type':types}\n",
    "data = pd.DataFrame(raw_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/home/bio_kang/data/top250_films.csv', index=None, encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "essential step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# the function is designed to get url information\n",
    "def get_url_info(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'}\n",
    "    try:\n",
    "        time.sleep(random.randint(1,2))\n",
    "        info = requests.get(url, headers=headers).text\n",
    "        soup = BeautifulSoup(info, 'html.parser')\n",
    "        return soup\n",
    "    except:\n",
    "        print('Sorry! The film information is not got')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download page information (just run one time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    time.sleep(random.randint(1,3))\n",
    "    page_info = get_url_info('https://movie.douban.com/top250?start={}&filter='.format(i*25))\n",
    "    with open('/home/bio_kang/Learning/Python/film_project/film_page_{}.html'.format(i), 'w') as f:\n",
    "        f.write(str(page_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all film urls, the number of people who voted and the score of all films of all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information\n",
    "urls = [] # each film url\n",
    "votes = [] # the vote people number\n",
    "scores = [] # the score of film\n",
    "names = [] # film name\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    with open('/home/bio_kang/Learning/Python/film_project/film_page_{}.html'.format(i), 'r') as f:\n",
    "        page_info = f.read()\n",
    "\n",
    "        pattern_url = re.compile(r'<a class=\"\" href=\"(.*?)\">')\n",
    "        pattern_vote = re.compile(r'<span>(\\d+.*?)</span>')\n",
    "        pattern_score = re.compile(r'<span class=\"rating_num\" property=\"v:average\">(.*?)</span>')\n",
    "        pattern_name = re.compile('<span class=\"title\">([^\\s].*?)</span>')\n",
    "\n",
    "        urls = urls + re.findall(pattern_url, page_info)\n",
    "        votes = votes + re.findall(pattern_vote, page_info)\n",
    "        scores = scores + re.findall(pattern_score, page_info)\n",
    "        names = names + re.findall(pattern_name, page_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    with open('/home/bio_kang/Learning/Python/film_project/download.csv','r') as f: # overlop write\n",
    "        downloads = f.read().strip('\\n').split('\\n')\n",
    "        if url in downloads:\n",
    "            return \"False\"\n",
    "        else:\n",
    "            with open('/home/bio_kang/Learning/Python/film_project/download.csv', 'r+') as k:\n",
    "                url_add = k.read().strip('\\n').split('\\n')\n",
    "                url_add = list(set(downloads))\n",
    "                url_add.append(url)\n",
    "            with open('/home/bio_kang/Learning/Python/film_project/download.csv', 'w') as j:\n",
    "                j.write(\"\\n\".join(url_add))\n",
    "            return \"True\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_votes = []\n",
    "for vote in votes:\n",
    "    pattern = re.compile(r'\\d+')\n",
    "    new_votes.append(str(re.findall(pattern, vote)).strip('[]').strip('\\''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloads and check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everyone film information of html\n",
    "for url in urls:\n",
    "    if check(url) == 'True':\n",
    "        time.sleep(random.randint(1,3))\n",
    "        print('the program have started downloading {} film'.format(urls.index(url)))\n",
    "        film_info = get_url_info(url)\n",
    "        with open('/home/bio_kang/Learning/Python/film_project/film_info/film_{}.html'.format(urls.index(url)), 'w') as f:\n",
    "            f.write(str(film_info))\n",
    "            print('the program finish downloading {} film'.format(urls.index(url)))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uni_num = []\n",
    "years = []\n",
    "countries = []\n",
    "directors = []\n",
    "actors = []\n",
    "descriptions = []\n",
    "for i in range(250):\n",
    "    with open('/home/bio_kang/Learning/Python/film_project/film_info/film_{}.html'.format(i), 'r') as f:\n",
    "        film_info = f.read()\n",
    "\n",
    "        pattern_uni_num = re.compile(r'<span class=\"pl\">IMDb:</span> (.*?)<br/>') # unique number\n",
    "        pattern_year = re.compile(r'<span class=\"year\">\\((.*?)\\)</span>') # year\n",
    "        pattern_country = re.compile(r'<span class=\"pl\">制片国家/地区:</span>(.*?)<br/>') # country\n",
    "        pattern_director = re.compile(r'<meta content=(.*?) property=\"video:director\"/>') # director\n",
    "        pattern_actor = re.compile(r'<meta content=\"(.*?)\" property=\"video:actor\"/>') # actors\n",
    "        pattern_description = re.compile(r'<meta content=\"(.*?)property=\"og:description\">') # description\n",
    "\n",
    "        uni_num.append(str(re.findall(pattern_uni_num, film_info)).strip(\"[]\").strip(\"'\"))\n",
    "        years.append(str(re.findall(pattern_year, film_info)).strip(\"[]\").strip(\"'\"))\n",
    "        countries.append(str(re.findall(pattern_country, film_info)).strip(\"[]\").strip(\"'\").split('/')[0])\n",
    "        directors.append(re.findall(pattern_director, film_info))\n",
    "        actors.append(re.findall(pattern_actor, film_info))\n",
    "        descriptions.append(str(re.findall(pattern_description, film_info)).strip('[]').strip('\\''))\n",
    "\n",
    "print(descriptions[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "with open('/home/bio_kang/Learning/Python/film_project/film_info/film_0.html', 'r') as f:\n",
    "    info = f.read()\n",
    "pattern = re.compile(r'<span property=\"v:summary\">(.*?)')\n",
    "print(re.findall(pattern, info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process directors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_director = []\n",
    "for director in directors:\n",
    "    if len(director) > 1:\n",
    "        new_director.append(str(director[0]).strip('[]').strip('\\'').strip('\\\"'))\n",
    "    else:\n",
    "        new_director.append(str(director).strip('[]').strip('\\'').strip('\\\"'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process actor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_actor = []\n",
    "for i in range(250):\n",
    "    if len(actors[i]) > 5:\n",
    "        new_actor.append(str(actors[i][:5]).strip('[]').strip('\\\"'))\n",
    "    else:\n",
    "        new_actor.append(str(actors[i]).strip('[]').strip('\\\"'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'encoding':uni_num, 'name':names, 'country':countries, 'director':new_director, 'actor':new_actor, 'vote':new_votes, 'score':scores, 'year':years, 'link':urls }\n",
    "data = pd.DataFrame(raw_data)\n",
    "data.to_csv('/home/bio_kang/Learning/Python/film_project/top250_film_info_new.csv', index=None, encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4c0b1c8e35d9f80793da27c1f09cec2906743a08f4271636815a29bc833a021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
